{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b0f14e75",
      "metadata": {
        "id": "b0f14e75"
      },
      "source": [
        "# Annotated Notebook for SciBERT Fine-Tuning for Text Classification\n",
        "\n",
        "This notebook demonstrates the complete workflow to fine-tune SciBERT for detecting automatically generated research abstracts using spaCy. Each block of code is thoroughly explained in Markdown to provide clarity on its purpose and functionality."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d0d3d35",
      "metadata": {
        "id": "7d0d3d35"
      },
      "source": [
        "## 1. Importing Libraries and Setting Up the Environment\n",
        "\n",
        "In this section, we import the necessary libraries for data manipulation, model training, and evaluation. We also verify the spaCy version to ensure compatibility."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy-transformers"
      ],
      "metadata": {
        "id": "avQ2pGoL_9g2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ffa880f-b55c-41d2-facf-4c7e4b001a0e"
      },
      "id": "avQ2pGoL_9g2",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spacy-transformers\n",
            "  Downloading spacy_transformers-1.3.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: spacy<4.1.0,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers) (3.7.5)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers) (1.26.4)\n",
            "Requirement already satisfied: transformers<4.50.0,>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers) (4.48.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers) (2.5.1+cu124)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers) (2.5.1)\n",
            "Collecting spacy-alignments<1.0.0,>=0.7.2 (from spacy-transformers)\n",
            "  Downloading spacy_alignments-0.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (1.1.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4.1.0,>=3.5.0->spacy-transformers) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->spacy-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->spacy-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->spacy-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->spacy-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->spacy-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->spacy-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->spacy-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->spacy-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->spacy-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->spacy-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->spacy-transformers) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers) (0.28.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers) (0.5.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.1.0,>=3.5.0->spacy-transformers) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<4.1.0,>=3.5.0->spacy-transformers) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy-transformers) (0.1.2)\n",
            "Downloading spacy_transformers-1.3.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (756 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.2/756.2 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading spacy_alignments-0.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.0/314.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: spacy-alignments, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, spacy-transformers\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 spacy-alignments-0.9.1 spacy-transformers-1.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5b5e4d2d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b5e4d2d",
        "outputId": "2e6c32d6-9ad3-4105-d9ac-dcb6506cdc43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spaCy version: 3.7.5\n"
          ]
        }
      ],
      "source": [
        "# Import key libraries\n",
        "import spacy\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Print spaCy version to verify compatibility\n",
        "print('spaCy version:', spacy.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c39660f",
      "metadata": {
        "id": "7c39660f"
      },
      "source": [
        "## 2. Dataset Preparation\n",
        "\n",
        "### 2.1 Loading and Inspecting Data\n",
        "\n",
        "Here we load our dataset (assumed to be in CSV format) which contains research abstracts labeled as either \"human_written\" or \"machine_generated\". We print the shape and the first few rows of the dataframe to confirm that the data has been loaded correctly."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/vijini/GeneratedTextDetection/archive/refs/heads/main.zip\n",
        "!unzip main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnscUMTzGKAc",
        "outputId": "418ebe96-f42b-4aa7-c335-5d9fc8e837d0"
      },
      "id": "OnscUMTzGKAc",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-27 22:21:40--  https://github.com/vijini/GeneratedTextDetection/archive/refs/heads/main.zip\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/vijini/GeneratedTextDetection/zip/refs/heads/main [following]\n",
            "--2025-02-27 22:21:41--  https://codeload.github.com/vijini/GeneratedTextDetection/zip/refs/heads/main\n",
            "Resolving codeload.github.com (codeload.github.com)... 20.205.243.165\n",
            "Connecting to codeload.github.com (codeload.github.com)|20.205.243.165|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘main.zip’\n",
            "\n",
            "main.zip                [      <=>           ] 800.25K   712KB/s    in 1.1s    \n",
            "\n",
            "2025-02-27 22:21:43 (712 KB/s) - ‘main.zip’ saved [819461]\n",
            "\n",
            "Archive:  main.zip\n",
            "ab034465f857a93212a894fe598edb749345b6ff\n",
            "   creating: GeneratedTextDetection-main/\n",
            "  inflating: GeneratedTextDetection-main/BLEU_sentence.py  \n",
            "   creating: GeneratedTextDetection-main/Dataset/\n",
            "   creating: GeneratedTextDetection-main/Dataset/FullyGenerated/\n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.09779_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.09779_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.10319_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.10319_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.10329_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.10329_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.10340_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.10340_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.10478_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.10478_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.10575_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.10575_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.10577_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.10577_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.10778_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.10778_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.10817_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.10817_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.11115_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.11115_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.11205_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.11205_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.11207_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.11207_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.11589_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.11589_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.11879_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.11879_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.11984_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.11984_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.12010_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.12010_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.12341_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.12341_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.12383_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.12383_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.12501_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.12501_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.12552_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.12552_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.12645_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.12645_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.12765_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.12765_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.13229_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.13229_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.13317_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.13317_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.13472_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.13472_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.13658_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.13658_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.13900_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.13900_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.14532_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.14532_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.15023_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.15023_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.15130_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.15130_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.15317_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.15317_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.15534_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.15534_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.15705_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.15705_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.15707_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.15707_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.15724_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.15724_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.15725_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.15725_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.15799_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.15799_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.15802_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2110.15802_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.00035_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.00035_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.00086_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.00086_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.00180_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.00180_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.00310_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.00310_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.00514_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.00514_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.00526_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.00526_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.00554_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.00554_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.00572_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.00572_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.00607_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.00607_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.00667_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.00667_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.00808_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.00808_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.00867_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.00867_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.01023_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.01023_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.01231_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.01231_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.01243_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.01243_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.01322_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.01322_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.01340_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.01340_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.01515_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.01515_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.01676_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.01676_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.01706_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.01706_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.02041_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.02041_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.02110_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.02110_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.02188_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.02188_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.02259_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.02259_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.02326_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.02326_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.02362_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.02362_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.02574_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.02574_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.02643_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.02643_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.02687_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.02687_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.02760_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.02760_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.02844_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.02844_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.03294_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.03294_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.03320_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.03320_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.03612_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.03612_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.03715_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.03715_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.03800_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.03800_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.03837_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.03837_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.03913_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.03913_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.03945_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.03945_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.04130_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.04130_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.04416_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.04416_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.04507_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.04507_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.04574_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.04574_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.05204_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.05204_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.05241_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.05241_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.05754_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.05754_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.06012_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.06012_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.06181_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.06181_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.06230_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.06230_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.06464_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.06464_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.06580_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.06580_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.06644_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.06644_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.07267_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.07267_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.07408_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.07408_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.07525_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.07525_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.07611_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.07611_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.07699_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.07699_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.07793_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.07793_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.15093_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.15093_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.15436_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.15436_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.15473_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2111.15473_original.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2112.00405_generated.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/FullyGenerated/2112.00405_original.txt  \n",
            " extracting: GeneratedTextDetection-main/Dataset/FullyGenerated/data  \n",
            "   creating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/\n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.09381_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.09381_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.09388_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.09388_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.09412_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.09412_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.09478_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.09478_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.09739_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.09739_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.09794_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.09794_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.09851_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.09851_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.09884_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.09884_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.09939_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.09939_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10044_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10044_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10056_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10056_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10280_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10280_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10291_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10291_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10297_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10297_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10309_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10309_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10452_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10452_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10476_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10476_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10488_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10488_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10501_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10501_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10518_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10518_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10522_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10522_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10545_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10545_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10595_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10595_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10622_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10622_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10625_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10625_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10627_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10627_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10734_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10734_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10772_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10772_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10831_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10831_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10832_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10832_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10847_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10847_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10896_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10896_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10897_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10897_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10898_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10898_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10970_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.10970_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11032_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11032_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11090_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11090_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11107_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11107_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11129_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11129_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11138_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11138_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11146_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11146_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11153_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11153_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11207_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11207_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11212_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11212_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11218_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11218_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11223_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11223_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11250_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11250_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11276_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11276_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11293_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11293_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11295_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11295_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11335_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11335_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11358_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11358_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11402_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11402_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11510_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11510_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11523_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11523_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11525_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11525_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11588_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11588_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11646_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11646_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11647_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11647_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11710_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11710_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11720_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11720_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11755_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11755_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11771_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11771_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11773_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11773_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11856_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11856_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11863_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11863_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11869_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11869_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11964_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11964_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11982_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.11982_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.12024_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.12024_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.12055_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.12055_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.12144_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.12144_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.12170_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.12170_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.12197_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.12197_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.12202_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.12202_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.12210_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.12210_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.12454_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.12454_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.12490_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.12490_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.12498_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.12498_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.12560_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.12560_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.12602_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.12602_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.12606_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.12606_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.12679_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.12679_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.12906_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.12906_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.12978_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.12978_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.12986_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.12986_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.13027_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.13027_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.13069_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.13069_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.13122_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.13122_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.13142_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.13142_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.13144_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.13144_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.13145_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.13145_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.13188_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.13188_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.13295_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.13295_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.13326_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.13326_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.13447_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.13447_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.13463_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.13463_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.13550_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.13550_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.13585_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.13585_originalAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.13654_generatedAbstract.txt  \n",
            "  inflating: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/2111.13654_originalAbstract.txt  \n",
            " extracting: GeneratedTextDetection-main/Dataset/Hybrid_AbstractDataset/data  \n",
            "  inflating: GeneratedTextDetection-main/README.md  \n",
            "  inflating: GeneratedTextDetection-main/n-gram_BLEU.py  \n",
            "  inflating: GeneratedTextDetection-main/rouge.py  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## We use the fully generated dataset\n",
        "dataset_path = Path(\"GeneratedTextDetection-main/Dataset/FullyGenerated\")\n",
        "\n",
        "texts, labels = [], []\n",
        "\n",
        "for original_file in dataset_path.glob(\"*_original.txt\"):\n",
        "    generated_file = original_file.with_name(original_file.stem.replace(\"original\", \"generated\") + \".txt\")\n",
        "\n",
        "    texts.append(original_file.read_text(encoding=\"utf-8\"))\n",
        "    labels.append(\"human_written\")\n",
        "\n",
        "    texts.append(generated_file.read_text(encoding=\"utf-8\"))\n",
        "    labels.append(\"machine_generated\")\n",
        "\n",
        "df = pd.DataFrame({\"text\": texts, \"label\": labels})\n",
        "df.to_csv(\"dataset.csv\", index=False)"
      ],
      "metadata": {
        "id": "J8DbFVxkGNOo"
      },
      "id": "J8DbFVxkGNOo",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "94f2271b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94f2271b",
        "outputId": "ea1f2008-4537-418c-d0fb-91d8b212b9a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (200, 2)\n",
            "First few entries:\n",
            "                                                 text              label\n",
            "0  ﻿Abstract We present the task of Automated Pun...      human_written\n",
            "1  Abstract We present the task of Automated Puni...  machine_generated\n",
            "2  ﻿Abstract Pre-trained language models (PLM) ha...      human_written\n",
            "3  Abstract Pre-trained language models (PLM) hav...  machine_generated\n",
            "4  ﻿Abstract Improving user experience of a dialo...      human_written\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(\"dataset.csv\")\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"First few entries:\\n\", df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f82c7337",
      "metadata": {
        "id": "f82c7337"
      },
      "source": [
        "### 2.2 Splitting the Dataset\n",
        "\n",
        "We split the data into training (80%) and testing (20%) sets, using stratification on the labels to maintain class balance. The resulting dataframes are saved as CSV files for later conversion into spaCy's binary format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4f12b9b4",
      "metadata": {
        "id": "4f12b9b4"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into training and testing sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df[\"text\"], df[\"label\"], test_size=0.2, random_state=42, stratify=df[\"label\"]\n",
        ")\n",
        "\n",
        "# Create DataFrames for train and test sets\n",
        "train_df = pd.DataFrame({\"text\": train_texts, \"label\": train_labels})\n",
        "test_df = pd.DataFrame({\"text\": test_texts, \"label\": test_labels})\n",
        "\n",
        "# Save the DataFrames to CSV files\n",
        "train_df.to_csv(\"train.csv\", index=False)\n",
        "test_df.to_csv(\"test.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a56e88b",
      "metadata": {
        "id": "9a56e88b"
      },
      "source": [
        "### 2.3 Converting Data to spaCy Format\n",
        "\n",
        "We now convert the CSV data into spaCy `Doc` objects. Using the `DocBin` utility, we serialize these documents into binary files (`train.spacy` and `test.spacy`) which are used later for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1b9b0ea1",
      "metadata": {
        "id": "1b9b0ea1"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.tokens import DocBin\n",
        "\n",
        "# Create a blank English model\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "def create_docbin(df):\n",
        "    doc_bin = DocBin()\n",
        "    for _, row in df.iterrows():\n",
        "        doc = nlp.make_doc(row[\"text\"])\n",
        "        # Initialize document categories\n",
        "        doc.cats = {\"human_written\": 0.0, \"machine_generated\": 0.0}\n",
        "        doc.cats[row[\"label\"]] = 1.0\n",
        "        doc_bin.add(doc)\n",
        "    return doc_bin\n",
        "\n",
        "# Convert the CSV data to spaCy binary format\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "create_docbin(train_df).to_disk(\"train.spacy\")\n",
        "create_docbin(test_df).to_disk(\"test.spacy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "744db6f8",
      "metadata": {
        "id": "744db6f8"
      },
      "source": [
        "## 3. Model Configuration and Training\n",
        "\n",
        "### 3.1 Configuring the Pipeline\n",
        "\n",
        "Our pipeline comprises two components:\n",
        "- **Transformer Component:** Loads the pretrained SciBERT model (`allenai/scibert_scivocab_uncased`) to generate contextual embeddings.\n",
        "- **Text Classification Component:** Uses spaCy’s `TextCatEnsemble.v2` architecture (with a bag-of-words submodel and Transformer listener) to classify texts as either human-written or machine-generated.\n",
        "\n",
        "The configuration for these components is stored in a separate file (`config.cfg`). Ensure that your `config.cfg` is set up as required before training. You can load it with this line also"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy init config config.cfg --lang en --pipeline transformer,textcat --optimize accuracy --gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hntMlegFIQ60",
        "outputId": "132cad01-eba8-45b4-8085-84129eaee834"
      },
      "id": "hntMlegFIQ60",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[38;5;1m✘ The provided output file already exists. To force overwriting the\n",
            "config file, set the --force or -F flag.\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then you need to replace the base model for the transformer with \"allenai/scibert_scivocab_uncased\" and provide the path for the training and test data"
      ],
      "metadata": {
        "id": "UkGPdeZ4I62R"
      },
      "id": "UkGPdeZ4I62R"
    },
    {
      "cell_type": "markdown",
      "id": "ac934b0a",
      "metadata": {
        "id": "ac934b0a"
      },
      "source": [
        "### 3.2 Training the Model\n",
        "\n",
        "To train the model, we run spaCy's training command. This command loads the configuration file, initializes the pipeline (including the SciBERT and text classification components), and starts the training process on the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "066453c5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "066453c5",
        "outputId": "e32efe25-8c3d-489a-d42c-87c59536d07d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Created output directory: output\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory: output\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "config.json: 100% 385/385 [00:00<00:00, 2.47MB/s]\n",
            "vocab.txt: 100% 228k/228k [00:00<00:00, 29.1MB/s]\n",
            "2025-02-27 22:22:16.217251: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1740694936.503654    1069 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1740694936.582086    1069 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-02-27 22:22:17.195113: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "pytorch_model.bin: 100% 442M/442M [00:01<00:00, 241MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/thinc/shims/pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(self._mixed_precision):\n",
            "model.safetensors: 100% 442M/442M [00:06<00:00, 64.6MB/s]\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['transformer', 'textcat']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
            "E    #       LOSS TRANS...  LOSS TEXTCAT  CATS_SCORE  SCORE \n",
            "---  ------  -------------  ------------  ----------  ------\n",
            "/usr/local/lib/python3.11/dist-packages/thinc/shims/pytorch.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(self._mixed_precision):\n",
            "/usr/local/lib/python3.11/dist-packages/spacy/training/corpus.py:102: UserWarning: [W090] Could not locate any .spacy files in path 'test./spacy'.\n",
            "  warnings.warn(Warnings.W090.format(path=orig_path, format=file_type))\n",
            "  0       0           0.00          0.25        0.00    0.00\n",
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: grad and param do not obey the gradient layout contract. This is not an error, but may impair performance.\n",
            "grad.sizes() = [768, 3072], strides() = [1, 768]\n",
            "param.sizes() = [768, 3072], strides() = [3072, 1] (Triggered internally at ../torch/csrc/autograd/functions/accumulate_grad.h:218.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "  1     200           0.00         50.42        0.00    0.00\n",
            "  3     400           0.02         41.14        0.00    0.00\n",
            "  4     600           0.00         14.06        0.00    0.00\n",
            "  6     800           0.00         13.70        0.00    0.00\n",
            "  7    1000           0.00         16.78        0.00    0.00\n",
            "  9    1200           0.00         11.04        0.00    0.00\n",
            " 10    1400           0.00         14.16        0.00    0.00\n",
            " 12    1600           0.00         11.83        0.00    0.00\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "output/model-last\n"
          ]
        }
      ],
      "source": [
        "# Execute the training command (this cell is for documentation; run in terminal or as a shell cell)\n",
        "!python -m spacy train config.cfg --output ./output --gpu-id 0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee0435e7",
      "metadata": {
        "id": "ee0435e7"
      },
      "source": [
        "## 4. Model Evaluation\n",
        "\n",
        "### 4.1 Evaluating on the Test Set\n",
        "\n",
        "After training, we evaluate the model on the test set using spaCy's evaluation command. This provides key metrics such as accuracy, precision, recall, and F1-score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4d9e63a1",
      "metadata": {
        "id": "4d9e63a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b24a1a32-dda5-4e8e-afaf-2b73b93ec512"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[38;5;4mℹ To switch to GPU 0, use the option: --gpu-id 0\u001b[0m\n",
            "2025-02-27 22:38:47.565097: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1740695927.586168    5185 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1740695927.592345    5185 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-02-27 22:38:47.615325: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/spacy_transformers/layers/hf_shim.py:124: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self._model.load_state_dict(torch.load(filelike, map_location=device))\n",
            "/usr/local/lib/python3.11/dist-packages/thinc/shims/pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(self._mixed_precision):\n",
            "\u001b[1m\n",
            "================================== Results ==================================\u001b[0m\n",
            "\n",
            "TOK                 100.00\n",
            "TEXTCAT (macro F)   89.90 \n",
            "SPEED               208   \n",
            "\n",
            "\u001b[1m\n",
            "=========================== Textcat F (per label) ===========================\u001b[0m\n",
            "\n",
            "                         P        R       F\n",
            "human_written        83.33   100.00   90.91\n",
            "machine_generated   100.00    80.00   88.89\n",
            "\n",
            "\u001b[1m\n",
            "======================== Textcat ROC AUC (per label) ========================\u001b[0m\n",
            "\n",
            "                    ROC AUC\n",
            "human_written          0.93\n",
            "machine_generated      0.93\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the trained model on the test set\n",
        "!python -m spacy evaluate ./output/model-best test.spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8200e739",
      "metadata": {
        "id": "8200e739"
      },
      "source": [
        "### 4.2 Inference on a Sample Text\n",
        "\n",
        "We load the trained model and perform inference on a sample abstract to see the predicted class probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "c8f15c4d",
      "metadata": {
        "id": "c8f15c4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "754e1628-ed6e-4b20-ac9f-a4cbf566e553"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/spacy_transformers/layers/hf_shim.py:124: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self._model.load_state_dict(torch.load(filelike, map_location=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class probabilities: {'human_written': 0.9976346492767334, 'machine_generated': 0.002365301363170147}\n",
            "Predicted label: human_written\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/thinc/shims/pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(self._mixed_precision):\n"
          ]
        }
      ],
      "source": [
        "# Load the best model\n",
        "nlp = spacy.load(\"./output/model-best\")\n",
        "\n",
        "# Sample text for inference\n",
        "sample_text = \"Insert a sample research abstract here to test model predictions.\"\n",
        "doc = nlp(sample_text)\n",
        "print(\"Predicted class probabilities:\", doc.cats)\n",
        "\n",
        "# Determine predicted label based on a threshold (e.g., 0.5)\n",
        "predicted_label = \"human_written\" if doc.cats.get(\"human_written\", 0) > 0.5 else \"machine_generated\"\n",
        "print(\"Predicted label:\", predicted_label)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#let's evaluate it on our test dataset\n",
        "human_text = test_df[\"text\"][0]\n",
        "doc = nlp(human_text)\n",
        "print(doc.cats) #prdict to machine generated (right label)\n",
        "\n",
        "machine_text = test_df[\"text\"][1]\n",
        "doc = nlp(machine_text)\n",
        "print(doc.cats) #predict to human written (right label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epHHeNr4EExu",
        "outputId": "aef9c35b-5f72-431d-faf8-420832beb342"
      },
      "id": "epHHeNr4EExu",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'human_written': 0.9976348876953125, 'machine_generated': 0.0023650594521313906}\n",
            "{'human_written': 1.1979941518802661e-05, 'machine_generated': 0.9999880790710449}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Deeper on the evaluation"
      ],
      "metadata": {
        "id": "3POPVk2OVsNl"
      },
      "id": "3POPVk2OVsNl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block of code performs an error analysis by identifying misclassified examples from the test dataset. Here’s what it does step by step.\n",
        "\n",
        "This analysis helps us pinpoint specific cases where the model's predictions differ from the ground truth, which can be useful for understanding errors and guiding further improvements.\n"
      ],
      "metadata": {
        "id": "_nYC-2MoaGPI"
      },
      "id": "_nYC-2MoaGPI"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a list to collect misclassified examples\n",
        "misclassified = []\n",
        "\n",
        "# Iterate over each test document\n",
        "for idx, row in test_df.iterrows():\n",
        "    text = row[\"text\"]\n",
        "    true_label = row[\"label\"]\n",
        "\n",
        "    # Process the text with the model\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Determine predicted label based on a threshold (0.5 here)\n",
        "    pred_label = \"human_written\" if doc.cats.get(\"human_written\", 0) > 0.5 else \"machine_generated\"\n",
        "\n",
        "    # If the prediction doesn't match the true label, store it\n",
        "    if pred_label != true_label:\n",
        "        misclassified.append({\n",
        "            \"index\": idx,\n",
        "            \"text\": text,\n",
        "            \"true_label\": true_label,\n",
        "            \"predicted_label\": pred_label,\n",
        "            \"probabilities\": doc.cats  # Optionally include the probability scores\n",
        "        })\n",
        "\n",
        "# Report the results\n",
        "print(f\"Found {len(misclassified)} misclassified examples.\\n\")\n",
        "for example in misclassified:\n",
        "    print(f\"Index: {example['index']}\")\n",
        "    print(f\"True Label: {example['true_label']} | Predicted Label: {example['predicted_label']}\")\n",
        "    print(f\"Probabilities: {example['probabilities']}\")\n",
        "    print(f\"Text: {example['text']}\")\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qO0dffsOV0jr",
        "outputId": "cd8ed56c-1f88-49d9-e279-39a55ce536ce"
      },
      "id": "qO0dffsOV0jr",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4 misclassified examples.\n",
            "\n",
            "Index: 8\n",
            "True Label: machine_generated | Predicted Label: human_written\n",
            "Probabilities: {'human_written': 0.9976348876953125, 'machine_generated': 0.002365087391808629}\n",
            "Text: Abstract Machine translation (MT) system aims to translate source language into target language. Recent studies on MT systems mainly focus on neural machine translation (NMT). One fac- tor that significantly affects the performance of NMT is the availability of high-quality paral- lel corpora. However, high-quality parallel cor- pora concerning Korean are relatively scarce compared to those associated with other high- resource languages, such as German or Italian. To address this problem, AI Hub recently re- leased seven types of parallel corpora for Ko- rean. In this study, we conduct an in-depth ver- ification of the quality of corresponding par- allel corpora through Linguistic Inquiry and Word Count (LIWC) and several relevant ex- periments. LIWC is a word-counting software program that can analyze corpora in multiple ways and extract linguistic features as a dictio- nary base. To the best of our knowledge, this study is the first to use LIWC to analyze par- allel corpora in the field of NMT. Our find- ings suggest the direction of further research toward obtaining the improved quality paral- lel corpora through our correlation analysis in LIWC and NMT performance. Introduction In recent years, the demand for machine translation (MT) systems has been continuously increasing and its importance is growing, especially for the industrial services. (Vieira et al., 2021; Zheng et al., 2019) Companies, such as Google, Facebook, Mi- crosoft, Amazon, and Unbabel continue to conduct research and formulate plans to commercialize ap- plications related to MT. From the late 1950s, numerous MT-related projects were proceeded by mainly focusing on rule-based and statistical-based approaches before the advent of deep learning technology. As deep learning based neural machine translation (NMT) was proposed and adopted to several researches, it has been gradually figured out that more supe- rior performance can be derived through NMT ap- proach (Bahdanau et al., 2014; Vaswani et al., 2017;Lample and Conneau, 2019; Song et al., 2019). Followed by the adoption of deep learning based technique, the improvements of computing power (e.g. GPU) and corresponding enhancement of par- allel processing accelerated the advancement of NMT. Recently, release of open source frameworks, such as Pytorch(Paszke et al., 2019), and lowered accessibility to the big data further facilitated vig- orous and diverse research. However, several issues considering the enhance- ment of the NMT system remain still. Represen- tatively, limitations in ensuring the quality of data is an unresolved issue. As have previously been studied, the quality of the training data is deeply related to the NMT performance (Park et al., 2020d, 2021b). The major problem is that the process of building a high-quality parallel corpus is time- consuming and expensive, and it is significantly difficult for low-resource languages, such as Ko- rean. Although data-augmentation techniques, such as back translation (Edunov et al., 2018) and copied translation (Currey et al., 2017) have been intro- duced, as the human supervision is generally mini- mized or excluded in the data generation process, the quality of such pseudo-generated parallel cor- pus cannot be guaranteed (Burlot and Yvon, 2019; Epaliyana et al., 2021). This restricted the usage of pseudo-generated parallel to complements of human-labeled gold parallel corpus, rather than its substitutes (Imankulova et al., 2017). For the alleviation of above limitations, numer- ous studies on the collection of high-quality train- ing data have been conducted, such as parallel cor- pus filtering (PCF) research and Data Dam project. PCF refers to a research field that aims to filter out low-quality noisy data (i.e. sentence pairs) residing in the parallel corpus, and improve the over- all quality of the corpus. PCF is currently being applied to various NMT studies and contributed to the advancement of the NMT systems (Koehn et al., 2019; Park et al., 2020c). While the amount of training data caused significant impact on the statistical-based MT approaches, the quality of data is treated as more important than the amount of data in general deep learning-based MT approaches (Khayrallah and Koehn, 2018; Koehn et al., 2020b). Moreover, Data Dam 1 projectsbuilding high- quality parallel corpora nationally are in progress. In the Republic of Korea, a large number of paral- lel corpora is open to the public through AI-Hub 2, which is organized by the National Information Society Agency (NIA) (Park and Lim, 2020). Following these research trends, where the qual- ity is treated more importantly than the quantity in the process, we analyzed the above Korean-English parallel corpus distributed by AI-Hub. Despite its sufficient amount of data, the quality of corresponding corpus has not been confirmed clearly. This may restrict the uncon- strained utilization of such corpus in adoption to the NMT model, as low quality data may degrade the overall performance. In this study, we conducted several quality verification experiments including Linguistic Inquiry and Word Count (LIWC) (Pen- nebaker et al., 2001; Tausczik and Pennebaker, 2010), and clarified the quality and characteris- tics of such corpus. By analyzing various factors that can affect NMT performance, we proposed a method that can be applied in future research using the analysis results. LIWC is a text-analysis tool that automatically analyzes the number of words in a sentence and classifies words with similar meanings and sen- timental characteristics. LIWC extracts various interpersonal variables related to clinical, social, physiological, cognitive, psychological, and de- velopmental contexts that cannot be detected us- ing previous text-analysis programs. Additionally, LIWC comprises a variety of features for analyzing text. LIWC generally used to recognize linguistic markers for mental health study in Psychopathol- ogy such as detecting Narcissism(Holtzman et al., 2019), schizophrenia(Bae et al., 2021), bipolar dis- order(Sekulic ÃÂ et al., 2018). However, LIWC pro- vides various linguistic features, word count, gender bias and so on, so it can be used for various analyses. In this study, we use LIWC to analyze parallel corpora based on diverse properties. It is also first time to analyze corpus using LIWC. In addition, we conduct baseline translation ex- periments by training transformer-base model struc- ture (Vaswani et al., 2017) through all the parallel corpora given by AIhub. By analyzing MT per- formance of corresponding models, we propose further research directions on MT for the Korean language. The contributions of this study are as follows: For the first time, we conduct a deep data anal- ysis on AI-Hub data. To the best of our knowl- edge, this is the first time LIWC has been used to analyze corpora. This study acts as a mile- stone for further studies on NMT with respect to the Korean language. We conduct baseline translation experiments on all the data in the AI-Hub parallel corpus. Our experiments provide a foundation for fur- ther research on Korean-based NMT. We discovered that many factors might cause decreasing model performance, and we pro- vide the direction that those factors could be filtered through our correlation analysis be- tween LIWC and model performance. Conclusions In this work, we proceeded with a quality eval- uation of all the Korean-related parallel corpus, released by AI Hub. For the model-centric perfor- mance validation, we constructed a transformer based NMT model trained with each parallel cor- pus. Through quantitative and qualitative analysis of these NMT models, we point out some proba- ble limitations on constructing corpora. First, for learning NMT model well in specific field, the do- main corpora should contain various words and expressions in consideration of the excessive per- formance difference between domain and general corpora. Second, given the significant performance gap in terms of language direction, half of the paral- lel data to be built must be configured in the source language and the other half in the target language and then translated respectively. Away from the model-centric analysis, we en- couraged data-centric research through LIWC anal- ysis. We figured out the association between LIWC and model performance in terms of data filtering. Through this analysis, we suggested the direction of further work to improve model performance. The national level re-examination of the various standards and building processes should be made for the encouragement of AI data construction re- searches. In the future, we plan to investigate ef- ficient beam search strategies and new decoding methods by utilizing these AI Hub data. Also, to more accurately measure the model performance, we plan to build an official Korean-English test set.\n",
            "--------------------------------------------------------------------------------\n",
            "Index: 23\n",
            "True Label: machine_generated | Predicted Label: human_written\n",
            "Probabilities: {'human_written': 0.9976348876953125, 'machine_generated': 0.0023651293013244867}\n",
            "Text: Abstract Across many data domains, co-occurrence statis- tics about the joint appearance of objects are powerfully informative. By transforming unsu- pervised learning problems into decompositions of co-occurrence statistics, spectral algorithms provide transparent and efficient algorithms for posterior inference such as latent topic analysis and community detection. As object vocabularies grow, however, it becomes rapidly more expensive to store and run inference algorithms on co-occur- rence statistics. Rectifying co-occurrence, the key process to uphold model assumptions, becomes increasingly more vital in the presence of rare terms, but current techniques cannot scale to large vocabularies. We propose novel methods that si- multaneously compress and rectify co-occurrence statistics, scaling gracefully with the size of vo- cabulary and the dimension of latent space. We also present new algorithms learning latent vari- ables from the compressed statistics, and verify that our methods perform comparably to previous approaches on both textual and non-textual data. Introduction Understanding the underlying geometry of noisy and com- plex data is a fundamental problem of unsupervised learning. Probabilistic models explain data generation processes in terms of low-dimensional latent variables. Inferring a poste- rior distribution for these latent variables provides us with a compact representation for various exploratory analyses and downstream tasks (Bengio et al., 2013). However, exact inference is often intractable due to entangled interactions between the latent variables (Blei et al., 2003; Airoldi et al., 2008; A. Erosheva, 2003; Pritchard et al., 2000). Variational inference transforms the posterior approximation into an optimization problem over simpler distributions with in- dependent parameters (Jordan et al., 1999; Wainwright & Jordan, 2008; Blei et al., 2017), while Markov Chain Monte Carlo enables users to sample from the desired posterior distribution (Neal, 1993; Neal et al., 2011; Robert & Casella, 2013). However, these likelihood-based methods require numerous iterations without any guarantee beyond local improvement at each step (Kulesza et al., 2014). When the data consists of collections of discrete objects, co-occurrence statistics summarize interactions between objects. Collaborative filtering learns low-dimensional rep- resentations of individual items, which are useful for recom- mendation systems, by explicitly decomposing the co-oc- currence of items that are jointly consumed by certain users (Lee et al., 2015; Liang et al., 2016). Word-vector mod- els learn low-dimensional embeddings of individual words, which encode useful linguistic biases for neural networks, by implicitly decomposing the co-occurrence of words that appear together in contexts (Pennington et al., 2014; Levy & Goldberg, 2014). If co-occurrence provides a rich enough set of unbiased moments about an underlying generative model, spectral methods can provably learn posterior con- figurations from co-occurrence information alone, without iterating through individual training examples (Arora et al., 2013; Anandkumar et al., 2012c; Hsu et al., 2012; Anand- kumar et al., 2012b). However, two major limitations hinder users from taking advantage of spectral inference based on co-occurrence. First, the second-order co-occurrence matrix already grows quadratically in the number of words (e.g. objects, items, products). Pruning the vocabulary is an option, but for a retailer selling millions of long-tailed products, learning representations of only a subset of the products is inade- quate. Second, inference quality is poor in real data that does not necessarily follow our generative model. Whereas likelihood-based methods (Blei et al., 2003; Airoldi et al., 2008; A. Erosheva, 2003; Pritchard et al., 2000) have an intrinsic capability to fit the data to the model despite their mismatch, sample noise can easily destroy the performance of spectral methods even if the data is synthesized from the model (Kulesza et al., 2014; Lee et al., 2015). Rectification, a process of projecting empirical co-occur- rence onto a manifold consistent with the posterior geometry of the model, provides a principled treatment that improves the performance of spectral inference in the face of model- data mismatch (Lee et al., 2015). Alternating Projection rectification (AP) has been used to rectify the input co- occurrence matrix to the Anchor Word algorithm (AW), a second-order spectral topic model (Leeal., 2015; 2017; 2019), but running multiple projections dominates overall inference cost even when the vocabulary is small. AP makes the co-occurrence dense as well, exacerbating storage costs when operating on large vocabularies. In this paper, we propose two efficient methods that si- multaneously compress and rectify the co-occurrence ma- trix, scaling gracefully with the size of vo- cabulary and the dimension of latent space. We also propose a new approach that learns posterior con- figurations from the compressed statistics, enabling users to better interpret noisy and com- plex data even if the data is synthesized from the compressed statistics. Our experiments show that applying the p- ulary algorithm on a corpus of jade-white words grows the size of a football field even when the words are synthesized from the compressed statistics, rendering the data useless for neural networks. We also present new algorithms learning latent vari- ables from the compressed statistics, and verify that our methods perform comparably to previous approaches on both textual and non-textual data. Introduction Understanding the underlying geometry of noisy and com- plex data is a fundamental problem of unsupervised learning. Probabilistic models explain data generation processes in terms of low-dimensional latent variables. Inferring a poste- rior distribution for these latent variables provides us with a compact representation for various exploratory analyses and downstream tasks (Bengio et al., 2013). However, exact inference is often intractable due to entangled interactions between the latent variables (Blei et al., 2003; Airoldi et al., 2008; A. Erosheva, 2003; Pritchard et al., 2000). Variational inference transforms the posterior approximation into an optimization problem over simpler distributions with in- dependent parameters (Jordan et al., 1999; Wainwright & Jordan, 2008; Blei et al., 2017), while Markov Chain Monte Carlo enables users to sample from the desired posterior distribution (Neal, 1993; Neal et al., 2011; Robert & Casella, 2013). However, these likelihood-based methods require numerous iterations without any guarantee beyond local improvement at each step (Kulesza et al., 2014). When the data consists of collections of discrete objects, co-occurrence statistics summarize interactions between objects. Collaborative filtering learns low-dimensional rep- resentations of individual items, which are useful for recom- mendation systems, by explicitly decomposing the co-oc- currence of items that are jointly consumed by certain users (Lee et al., 2015; Liang et al., 2016). Word-vector mod- els learn low-dimensional embeddings of individual words, which encode useful linguistic biases for neural networks, by implicitly decomposing the co-occurrence of words that appear together in contexts (Pennington et al., 2014; Levy & Goldberg, 2014). If co-occurrence provides a rich enough set of unbiased moments about an underlying generative model, spectral methods can provably learn posterior con- figurations from co-occurrence information alone, without iterating through individual training examples (Arora et al., 2013; Anandkumar et al., 2012c; Hsu et al., 2012; Anand- kumar et al., 2012b). However, two major limitations hinder users from taking advantage of spectral inference based on co-occurrence. First, the second-order co-occurrence matrix already grows quadratically in the number of words (e.g. objects, items, products). Pruning the vocabulary is an option, but for a retailer selling millions of long-tailed products, learning representations of only a subset of the products is inade- quate. Second, inference quality is poor in real data that does not necessarily follow our generative model. Whereas likelihood-based methods (Blei et al., 2003; Airoldi et al., 2008; A. Erosheva, 2003; Pritchard et al., 2000) have an intrinsic capability to fit the data to the model despite their mismatch, sample noise can easily destroy the performance of spectral methods even if the data is synthesized from the model (Kulesza et al., 2014; Lee et al., 2015). Rectification, a process of projecting empirical co-occur- rence onto a manifold consistent with the posterior geometry of the model, provides a principled treatment that improves the performance of spectral inference in the face of model- data mismatch (Lee et al., 2015).\n",
            "--------------------------------------------------------------------------------\n",
            "Index: 25\n",
            "True Label: machine_generated | Predicted Label: human_written\n",
            "Probabilities: {'human_written': 0.997634768486023, 'machine_generated': 0.002365230582654476}\n",
            "Text: Abstract The development of neural networks for clini- cal artificial intelligence (AI) is reliant on in- terpretability, transparency, and performance. The need to delve into the black-box neural network and derive interpretable explanations of model output is paramount. A task of high clinical importance is predicting the likelihood of a patient being readmitted to hospital in the near future to enable efficient triage. With the increasing adoption of electronic health records (EHRs), there is great interest in applications of natural language processing (NLP) to clin- ical free-text contained within EHRs. In this work, we apply InfoCal (Sha et al., 2020), the current state-of-the-art model that produces ex- tractive rationales for its predictions, to the task of predicting hospital readmission using hospital discharge notes. We compare extrac- tive rationales produced by InfoCal to com- petitive transformer-based models pretrained on clinical text data and for which the atten- tion mechanism can be used for interpretation. We find each presented model with selected in- terpretability or feature importance methods yield varying results, with clinical language do- main expertise and pretraining critical to per- formance and subsequent interpretability. Keywords: Extractive rationales, NLP, adver- sarial training, hospital readmission, decision support Introduction The use of machine learning in clinical settings is be- coming widespread, and with the adoption of elec- tronic health records (EHRs) has produced big health data and been opened to modern machine learning approaches (Jiang et al., 2017; Vaci et al., 2020). Whilst ubiquitous, there is still a relative lack of up- take in the use of machine learning in live clinical settings, with neural networks often labelled black boxes. The transparency and explainability of neural networks is critical to trust and acceptance in clinical environments as useful tools. Research has sought to provide improved inter- pretability of NLP models using methods to derive extractive rationales for neural networks predictions by the model itself (Bastings et al., 2019; Lei et al., 2016; Sha et al., 2020). The aim of rationale produc- tion is to increase explainability of models by extract- ing the minimal crucial input required to make a class prediction. In NLP, the rationales are subsets of the input text which maintain predictive power. One of the ways rationales have been achieved is by creating a two-module network, i.e., a selector followed by a predictor, which are trained jointly (Lei et al., 2016). Given an input x, the selector picks a subset of the input features r(x) (the rationale) by specifying a dis- tribution over the possible rationales. The predictor acts as a standard classifier, taking as input r(x) and predicting a class yÃÂ to compare with the ground truth class y. Sha et al. (2020) proposed InfoCal, an improved type of selector-predictor model that uses an information calibration technique and an additional guider module trained jointly with the selector and predictor in an adversarial manner. InfoCal achieves the current state-of-the-art in rationale extraction on tasks such as sentiment analysis and legal judgment prediction, hence we are interested to see how it per- forms in the medical domain. In this work, we apply InfoCal to the task of predicting hospital readmission from EHRs (John- son et al., 2016). We compare InfoCal with clinical domain Bidirectional Encoder Transformer (BERT) models, ClinicalBERT (Huang et al., 2019) and Bio- ClinicalBERT (Alsentzer et al., 2019). Additionally we compare InfoCal exrractive rationales with impor- tance features in the BERT models via self-attention and layerwise relevance propagation (LRP). We find that the BERT models outperform InfoCal on the classification task, but has a relatively limited mech- anism for interpretability in the form of self-attention. InfoCal was able to produce extractive rationales which reach baseline performance on the classifica- tion task, and we argue the difficulty lies in the do- main expertise created by pretraining present in the BERT based models. Related Work With the advent of big data and machine learning, research is beginning to glean insights from the many types of EHRs data (Li et al., 2020; Huang et al., 2019;et al., 2020; Weng et al., 2017; Wang et al., 2018; Kuruvilla and Gunavathi, 2014; Barak-Corren et al., 2017; Johnson et al., 2016). Data within EHRs can be either structured (following a pre-defined data structure and type, such as ECG recordings, x-ray images, laboratory results, and de- mographics) or unstructured data (lacking formal rules, type, and bounds, such as the free-text clin- ical free-text contained within EHRs). The frequency and volume in which clinical text is recorded for the first time is critical to trust and acceptance in clinical environments. In this work, we apply InfoCal (Sha et al., 2020), the current state-of-the-art model that produces ex- tractive rationales for its predictions, to the task of predicting hospital readmission using hospital discharge notes. We compare extrac- tive rationales produced by InfoCal to com- petitive transformer-based models pretrained on clinical text data and for which the atten- tion mechanism can be used for interpretation. We find each presented model with selected in- terpretability or feature importance methods yield varying results, with clinical language do- main expertise and pretraining critical to per- formance and subsequent interpretability.\n",
            "--------------------------------------------------------------------------------\n",
            "Index: 26\n",
            "True Label: machine_generated | Predicted Label: human_written\n",
            "Probabilities: {'human_written': 0.997634768486023, 'machine_generated': 0.0023651875089854}\n",
            "Text: ABSTRACT We propose BERMo, an architectural modification to BERT, which makes predic- tions based on a hierarchy of surface, syntactic and semantic language features. We use linear combination scheme proposed in Embeddings from Language Mod- els (ELMo) to combine the scaled internal representations from different network depths. Our approach has two-fold benefits: (1) improved gradient flow for the downstream task as every layer has a direct connection to the gradients of the loss function and (2) increased representative power as the model no longer needs to copy the features learned in the shallower layer which are necessary for the down- stream task. Further, our model has a negligible parameter overhead as there is a single scalar parameter associated with each layer in the network. Experiments on the probing task from SentEval dataset show that our model performs up to 4.65% better in accuracy than the baseline with an average improvement of 2.67% on the semantic tasks. When subject to compression techniques, we find that our model enables stable pruning for compressing small datasets like SST-2, where the BERT model commonly diverges. We observe that our approach converges 1.67ÃÂ and 1.15ÃÂ faster than the baseline on MNLI and QQP tasks from GLUE dataset. Moreover, our results show that our approach can obtain better parameter efficiency for penalty based pruning approaches on QQP task. INTRODUCTION The invention of Transformer (Vaswani et al. (2017)) architecture has paved new research direc- tions in the deep learning community. Descendants of this architecture, namely BERT (Devlin et al. (2019)) and GPT (Brown et al. (2020)), attain State-of-The-Art (SoTA) performance for a broad range of NLP applications. The success of these networks is primarily attributed to the two-stage training process (self-supervised pre-training and task-based fine-tuning), and the attention mech- anism introduced in Transformers. Many of the top models on various leader boards use models from the BERT family. All of the fifteen systems that surpass the human baseline on the Gen- eral Language Understanding Evaluation (GLUE) (Wang et al. (2018)) benchmark use variants of BERT or have it as one of the constituents in an ensemble, except for T5 (Raffel et al. (2019)), which uses the Transformer architecture. Further, the best-performing systems for each task in the Ontonotes (Weischedel et al. (2011)) benchmark belong to the BERT family, with the exception of Entity Typing task where Embeddings from Language Models (ELMo) (Peters et al. (2018)) tops the leaderboard. These promising results make the BERT family of models increasingly ubiquitous in solving several tasks in the various domains of machine learning like NLP, Image Recognition (Dosovitskiy et al. (2021); Jaegle et al. (2021)) and Object detection (Carion et al. (2020)). Motivation: In (Jawahar et al. (2019)), the authors manifest the lower layers of BERT to capture phrase-level information, which gets diluted with the depth. Moreover, they also demonstrate that the initial layers capture surface-level features, the middle layers deal with syntactic features, and the last few layers are responsible for semantic features. These findings indicate that BERT captures a rich hierarchy of linguistic features at different depths of the network. Intrigued by this discovery, we aim at combining the activations from different depths to obtain a richer feature representation. We find our problem formulation similar to the one presented in ELMo, where the authors illus- trate higher-level Long Short Term Memory (LSTM) states capture context-dependent aspects of word meaning or semantic features, and the lower-level LSTM states model the syntax. Inspired by ELMo, we propose BERMo by modifying the BERT architecture to increase the dependence of features from different depths to generate a rich context-dependent embedding. This approach improves the gradient flow during the backward pass and increases the representative power of the network (He et al. (2016); Huang et al. (2016)). Further, the linear combination of features from intermediate layers, proposed in ELMo, is simpler form of skip connection introduced in ResNets (He et al. (2016)). The skip connections in ResNets enable aggressive pooling in initial layers with- out affecting the gradient flow. This in turn leads to low parameters allowingnetworks to have orders of magnitude lower parameters compared to the architectures without skip connections such as VGG (Simonyan & Zisserman (2014)) while achieving competitive accuracy. Since the perfor- mance improvements associated with the BERT architecture come at the cost of a large memory footprint and enormous computational resources, compression becomes necessary to deploy these enormous resources on demand. As we introduce skip connections in the BERT architecture, the result is a large memory footprint and enormous computational resources. These enormous resources make the BERT ideal for distributing resource bruntings across different network depths. We observe that our approach converges 1.67ÃÂ and 1.15ÃÂ faster than the baseline on MNLI and QQP tasks from GLUE dataset. Moreover, our results show that our approach can obtain better parameter efficiency for penalty based pruning approaches on QQP task. INTRODUCTION The invention of Transformer (Vaswani et al. (2017)) architecture has paved new research direc- tions in the deep learning community. Descendants of this architecture, namely BERT (Devlin et al. (2019)) and GPT (Brown et al. (2020)), attain State-of-The-Art (SoTA) performance for a broad range of NLP applications. The success of these networks is primarily attributed to the two-stage training process (self-supervised pre-training and task-based fine-tuning), and the attention mech- anism introduced in Transformers. Many of the top models on various leader boards use models from the BERT family. All of the fifteen systems that surpass the human baseline on the Gen- eral Language Understanding Evaluation (GLUE) (Wang et al. (2018)) benchmark use variants of BERT or have it as one of the constituents in an ensemble, except for T5 (Raffel et al. (2019)), which uses the Transformer architecture. Further, the best-performing systems for each task in the Ontonotes (Weischedel et al. (2011)) benchmark belong to the BERT family, with the exception of Entity Typing task where Embeddings from Language Models (ELMo) (Peters et al. (2018)) tops the leaderboard. These promising results make the BERT family of models increasingly ubiquitous in solving several tasks in the various domains of machine learning like NLP, Image Recognition (Dosovitskiy et al. (2021); Jaegle et al. (2021)) and Object detection (Carion et al. (2020)). Motivation: In (Jawahar et al. (2019)), the authors manifest the lower layers of BERT to capture phrase-level information, which gets diluted with the depth. Moreover, they also demonstrate that the initial layers capture surface-level features, the middle layers deal with syntactic features, and the last few layers are responsible for semantic features. These findings indicate that BERT captures a rich hierarchy of linguistic features at different depths of the network. Intrigued by this discovery, we aim at combining the activations from different depths to obtain a richer feature representation.\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We can observe the model has only misclassify on machine generated text, highlighting the difficulty to detect machine generated as it compared to human generated text.\n",
        "\n",
        "* One other point is the high confidence score of the model on it's prediction, that is the point to investigate in future work.\n"
      ],
      "metadata": {
        "id": "j2dOIVhWY2on"
      },
      "id": "j2dOIVhWY2on"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dependency tree depth"
      ],
      "metadata": {
        "id": "usvkSJqla-Qr"
      },
      "id": "usvkSJqla-Qr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dependency Tree Depth Analysis\n",
        "\n",
        "This block of code calculates and visualizes the **average dependency tree depth** for sentences within documents. The dependency tree depth is a measure of the syntactic complexity of a sentence. In a dependency tree, each word is connected to its dependents (i.e., the words it governs). The depth of this tree represents the number of layers of dependencies from the root word to the most distant leaf node. A higher tree depth can indicate a more complex sentence structure, which might be a useful feature for distinguishing between human-written and machine-generated text.\n",
        "\n",
        "#### Code Breakdown:\n",
        "\n",
        "1. **Recursive Function `tree_depth`:**\n",
        "   - **Purpose:** Computes the depth of the dependency tree starting from a given token.\n",
        "   - **Mechanism:**  \n",
        "     - It recursively examines the children of the token.  \n",
        "     - If a token has no children, its depth is `1`.  \n",
        "     - Otherwise, it returns `1` plus the maximum depth among all its children.\n",
        "\n",
        "2. **Function `average_tree_depth`:**\n",
        "   - **Purpose:** Calculates the average dependency tree depth for all sentences in a document.\n",
        "   - **Mechanism:**  \n",
        "     - It iterates over each sentence in the document (using `doc.sents`).  \n",
        "     - For each sentence, it identifies the root token (the token whose head is itself) and computes its tree depth using the `tree_depth` function.  \n",
        "     - The function returns the average depth across all sentences in the document.\n",
        "\n",
        "3. **Sentence Segmentation:**\n",
        "   - The `sentencizer` component is added to the pipeline (`nlp.add_pipe(\"sentencizer\")`) to ensure that the document is properly segmented into sentences for analysis.\n",
        "\n",
        "4. **Data Collection and Plotting:**\n",
        "   - For a subset of test documents, the code computes the average dependency tree depth and stores these values in a dictionary (`depths_data`) categorized by the document label (either `\"human_written\"` or `\"machine_generated\"`).\n",
        "   - The collected depths are then transformed into a DataFrame (`depths_df`) and visualized using a boxplot. This plot compares the distribution of average tree depths across the two categories.\n",
        "\n",
        "#### Utility:\n",
        "\n",
        "By comparing the average dependency tree depths of human-written versus machine-generated texts, we can explore whether one class tends to have more syntactically complex sentences than the other. Such insights can inform further refinements in our classification approach or serve as additional features for downstream tasks.\n",
        "\n",
        "The full code block for this analysis is as follows:\n"
      ],
      "metadata": {
        "id": "9FAyIxGMb16T"
      },
      "id": "9FAyIxGMb16T"
    },
    {
      "cell_type": "code",
      "source": [
        "def tree_depth(token):\n",
        "    \"\"\"Recursively compute the depth of the dependency tree starting from the given token.\"\"\"\n",
        "    children = list(token.children)\n",
        "    if not children:\n",
        "        return 1\n",
        "    return 1 + max(tree_depth(child) for child in children)\n",
        "\n",
        "def average_tree_depth(doc):\n",
        "    \"\"\"Compute average dependency tree depth for all sentences in a document.\"\"\"\n",
        "    depths = []\n",
        "    for sent in doc.sents:\n",
        "        # Identify the root of the sentence\n",
        "        roots = [token for token in sent if token.head == token]\n",
        "        if roots:\n",
        "            depths.append(tree_depth(roots[0]))\n",
        "    return sum(depths)/len(depths) if depths else 0\n",
        "\n",
        "nlp.add_pipe(\"sentencizer\")\n",
        "\n",
        "# Collect average dependency tree depth for each text by label\n",
        "depths_data = {\"human_written\": [], \"machine_generated\": []}\n",
        "for idx, row in test_df.iterrows():\n",
        "    text = row[\"text\"]\n",
        "    label = row[\"label\"]\n",
        "    doc = nlp(text)\n",
        "    avg_depth = average_tree_depth(doc)\n",
        "    depths_data[label].append(avg_depth)\n",
        "\n",
        "# Create a DataFrame for plotting\n",
        "data = []\n",
        "for label in depths_data:\n",
        "    for d in depths_data[label]:\n",
        "        data.append({\"Label\": label, \"Average_Depth\": d})\n",
        "depths_df = pd.DataFrame(data)\n",
        "\n",
        "# Plot a boxplot comparing average dependency tree depth by label\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.boxplot(x=\"Label\", y=\"Average_Depth\", data=depths_df)\n",
        "plt.title(\"Average Dependency Tree Depth by Text Label\")\n",
        "plt.xlabel(\"Text Label\")\n",
        "plt.ylabel(\"Average Tree Depth\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "I0fhFkC2bCxx"
      },
      "id": "I0fhFkC2bCxx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two graphs are someway similar, making the classification task harder."
      ],
      "metadata": {
        "id": "baX1l5w0dRKr"
      },
      "id": "baX1l5w0dRKr"
    },
    {
      "cell_type": "markdown",
      "id": "efc10284",
      "metadata": {
        "id": "efc10284"
      },
      "source": [
        "## 5. Packaging and Deployment\n",
        "\n",
        "### 5.1 Packaging the Trained Model\n",
        "\n",
        "Once training is complete, we package the trained model into an installable distribution. The following command uses spaCy's packaging tool to generate a wheel file that can be uploaded to GitHub Releases or another hosting service. (the tar.gz files containing our model is also on the zip files of the projects, you can use it to directly use our model without training it first)\n",
        "\n",
        "```bash\n",
        "python -m spacy package ./output/model-best ./my_model_package --build wheel --create-meta\n",
        "```\n",
        "\n",
        "After packaging, locate the generated wheel file (inside the `dist/` folder of the created package directory) for distribution."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy package ./output/model-best ./my_model_package --build wheel --create-meta"
      ],
      "metadata": {
        "id": "9wT5jZERAZiC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7ce5a57-1951-4b2e-faa5-5847d1ec7831"
      },
      "id": "9wT5jZERAZiC",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;3m⚠ Generating packages without the 'build' package is deprecated and\n",
            "will not be supported in the future. To install 'build': pip install build\u001b[0m\n",
            "\u001b[38;5;4mℹ Building package artifacts: wheel\u001b[0m\n",
            "2025-02-27 22:56:17.144210: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1740696977.164471    9627 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1740696977.170539    9627 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-02-27 22:56:17.191226: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/spacy_transformers/layers/hf_shim.py:124: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self._model.load_state_dict(torch.load(filelike, map_location=device))\n",
            "\u001b[38;5;2m✔ Including 1 package requirement(s) from meta and config\u001b[0m\n",
            "spacy-transformers>=1.3.8,<1.4.0\n",
            "\u001b[1m\n",
            "============================ Generating meta.json ============================\u001b[0m\n",
            "Enter the package settings for your pipeline. The following information will be\n",
            "read from your pipeline data: pipeline, vectors.\n",
            "    Pipeline language (default: en):en\n",
            "    Pipeline name (default: pipeline):model_textcat_ngn\n",
            "    Package version (default: 0.0.0):0.1.0\n",
            "    Package description:\n",
            "    Author:\n",
            "    Author email:\n",
            "    Author website:\n",
            "    License:\n",
            "\n",
            "\u001b[38;5;2m✔ Generated README.md from meta.json\u001b[0m\n",
            "\u001b[38;5;2m✔ Successfully created package directory\n",
            "'en_model_textcat_ngn-0.1.0'\u001b[0m\n",
            "my_model_package/en_model_textcat_ngn-0.1.0\n",
            "/usr/bin/python3: No module named build\n",
            "\u001b[38;5;3m⚠ Creating wheel with 'python -m build' failed. Falling back to\n",
            "deprecated use of 'wheel' with 'python setup.py bdist_wheel'\u001b[0m\n",
            "running bdist_wheel\n",
            "running build\n",
            "running build_py\n",
            "creating build/lib/en_model_textcat_ngn\n",
            "copying en_model_textcat_ngn/__init__.py -> build/lib/en_model_textcat_ngn\n",
            "creating build/lib/en_model_textcat_ngn/en_model_textcat_ngn-0.1.0\n",
            "copying en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/config.cfg -> build/lib/en_model_textcat_ngn/en_model_textcat_ngn-0.1.0\n",
            "copying en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/README.md -> build/lib/en_model_textcat_ngn/en_model_textcat_ngn-0.1.0\n",
            "copying en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/tokenizer -> build/lib/en_model_textcat_ngn/en_model_textcat_ngn-0.1.0\n",
            "copying en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/meta.json -> build/lib/en_model_textcat_ngn/en_model_textcat_ngn-0.1.0\n",
            "creating build/lib/en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/transformer\n",
            "copying en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/transformer/model -> build/lib/en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/transformer\n",
            "\n",
            "copying en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/transformer/cfg -> build/lib/en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/transformer\n",
            "creating build/lib/en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/textcat\n",
            "copying en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/textcat/model -> build/lib/en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/textcat\n",
            "copying en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/textcat/cfg -> build/lib/en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/textcat\n",
            "creating build/lib/en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/vocab\n",
            "copying en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/vocab/key2row -> build/lib/en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/vocab\n",
            "copying en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/vocab/lookups.bin -> build/lib/en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/vocab\n",
            "copying en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/vocab/vectors.cfg -> build/lib/en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/vocab\n",
            "copying en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/vocab/strings.json -> build/lib/en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/vocab\n",
            "copying en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/vocab/vectors -> build/lib/en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/vocab\n",
            "copying en_model_textcat_ngn/meta.json -> build/lib/en_model_textcat_ngn\n",
            "/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "installing to build/bdist.linux-x86_64/wheel\n",
            "running install\n",
            "running install_lib\n",
            "creating build/bdist.linux-x86_64/wheel\n",
            "creating build/bdist.linux-x86_64/wheel/en_model_textcat_ngn\n",
            "creating build/bdist.linux-x86_64/wheel/en_model_textcat_ngn/en_model_textcat_ngn-0.1.0\n",
            "copying build/lib/en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/config.cfg -> build/bdist.linux-x86_64/wheel/./en_model_textcat_ngn/en_model_textcat_ngn-0.1.0\n",
            "creating build/bdist.linux-x86_64/wheel/en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/transformer\n",
            "copying build/lib/en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/transformer/model -> build/bdist.linux-x86_64/wheel/./en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/transformer\n",
            "copying build/lib/en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/transformer/cfg -> build/bdist.linux-x86_64/wheel/./en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/transformer\n",
            "creating build/bdist.linux-x86_64/wheel/en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/textcat\n",
            "copying build/lib/en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/textcat/model -> build/bdist.linux-x86_64/wheel/./en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/textcat\n",
            "copying build/lib/en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/textcat/cfg -> build/bdist.linux-x86_64/wheel/./en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/textcat\n",
            "creating build/bdist.linux-x86_64/wheel/en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/vocab\n",
            "copying build/lib/en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/vocab/key2row -> build/bdist.linux-x86_64/wheel/./en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/vocab\n",
            "copying build/lib/en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/vocab/lookups.bin -> build/bdist.linux-x86_64/wheel/./en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/vocab\n",
            "copying build/lib/en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/vocab/vectors.cfg -> build/bdist.linux-x86_64/wheel/./en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/vocab\n",
            "copying build/lib/en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/vocab/strings.json -> build/bdist.linux-x86_64/wheel/./en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/vocab\n",
            "copying build/lib/en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/vocab/vectors -> build/bdist.linux-x86_64/wheel/./en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/vocab\n",
            "copying build/lib/en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/README.md -> build/bdist.linux-x86_64/wheel/./en_model_textcat_ngn/en_model_textcat_ngn-0.1.0\n",
            "copying build/lib/en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/tokenizer -> build/bdist.linux-x86_64/wheel/./en_model_textcat_ngn/en_model_textcat_ngn-0.1.0\n",
            "copying build/lib/en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/meta.json -> build/bdist.linux-x86_64/wheel/./en_model_textcat_ngn/en_model_textcat_ngn-0.1.0\n",
            "copying build/lib/en_model_textcat_ngn/meta.json -> build/bdist.linux-x86_64/wheel/./en_model_textcat_ngn\n",
            "copying build/lib/en_model_textcat_ngn/__init__.py -> build/bdist.linux-x86_64/wheel/./en_model_textcat_ngn\n",
            "running install_egg_info\n",
            "running egg_info\n",
            "creating en_model_textcat_ngn.egg-info\n",
            "writing en_model_textcat_ngn.egg-info/PKG-INFO\n",
            "writing dependency_links to en_model_textcat_ngn.egg-info/dependency_links.txt\n",
            "writing entry points to en_model_textcat_ngn.egg-info/entry_points.txt\n",
            "writing requirements to en_model_textcat_ngn.egg-info/requires.txt\n",
            "writing top-level names to en_model_textcat_ngn.egg-info/top_level.txt\n",
            "writing manifest file 'en_model_textcat_ngn.egg-info/SOURCES.txt'\n",
            "reading manifest file 'en_model_textcat_ngn.egg-info/SOURCES.txt'\n",
            "reading manifest template 'MANIFEST.in'\n",
            "warning: no files found matching 'LICENSE'\n",
            "warning: no files found matching 'LICENSES_SOURCES'\n",
            "writing manifest file 'en_model_textcat_ngn.egg-info/SOURCES.txt'\n",
            "Copying en_model_textcat_ngn.egg-info to build/bdist.linux-x86_64/wheel/./en_model_textcat_ngn-0.1.0-py3.11.egg-info\n",
            "running install_scripts\n",
            "creating build/bdist.linux-x86_64/wheel/en_model_textcat_ngn-0.1.0.dist-info/WHEEL\n",
            "creating 'dist/en_model_textcat_ngn-0.1.0-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "adding 'en_model_textcat_ngn/__init__.py'\n",
            "adding 'en_model_textcat_ngn/meta.json'\n",
            "adding 'en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/README.md'\n",
            "adding 'en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/config.cfg'\n",
            "adding 'en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/meta.json'\n",
            "adding 'en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/tokenizer'\n",
            "adding 'en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/textcat/cfg'\n",
            "adding 'en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/textcat/model'\n",
            "adding 'en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/transformer/cfg'\n",
            "\n",
            "adding 'en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/transformer/model'\n",
            "adding 'en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/vocab/key2row'\n",
            "adding 'en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/vocab/lookups.bin'\n",
            "adding 'en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/vocab/strings.json'\n",
            "adding 'en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/vocab/vectors'\n",
            "adding 'en_model_textcat_ngn/en_model_textcat_ngn-0.1.0/vocab/vectors.cfg'\n",
            "adding 'en_model_textcat_ngn-0.1.0.dist-info/METADATA'\n",
            "adding 'en_model_textcat_ngn-0.1.0.dist-info/WHEEL'\n",
            "adding 'en_model_textcat_ngn-0.1.0.dist-info/entry_points.txt'\n",
            "adding 'en_model_textcat_ngn-0.1.0.dist-info/top_level.txt'\n",
            "adding 'en_model_textcat_ngn-0.1.0.dist-info/RECORD'\n",
            "removing build/bdist.linux-x86_64/wheel\n",
            "\u001b[38;5;2m✔ Successfully created binary wheel\u001b[0m\n",
            "my_model_package/en_model_textcat_ngn-0.1.0/dist/en_model_textcat_ngn-0.1.0-py3-none-any.whl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2a53246",
      "metadata": {
        "id": "b2a53246"
      },
      "source": [
        "### 5.2 Installing the Packaged Model on a Remote Machine\n",
        "\n",
        "To install the packaged model on a remote machine, upload the tar.gz file to GitHub Releases. Then, use the following command on the remote machine:\n",
        "\n",
        "\n",
        "This allows users to install your model directly via pip without having access to the local package folder."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install https://github.com/HermesNdjeng/nlp_project_ngn/releases/download/model/en_model_textcat_ngn-0.1.0.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqIzDF98TRJ2",
        "outputId": "32e3476f-5728-4610-b48a-a0b6654a5b92"
      },
      "id": "UqIzDF98TRJ2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting https://github.com/HermesNdjeng/nlp_project_ngn/releases/download/model/en_model_textcat_ngn-0.1.0.tar.gz\n",
            "  Downloading https://github.com/HermesNdjeng/nlp_project_ngn/releases/download/model/en_model_textcat_ngn-0.1.0.tar.gz (415.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m415.4/415.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: spacy<3.8.0,>=3.7.5 in /usr/local/lib/python3.11/dist-packages (from en_model_textcat_ngn==0.1.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-transformers<1.4.0,>=1.3.8 in /usr/local/lib/python3.11/dist-packages (from en_model_textcat_ngn==0.1.0) (1.3.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.5->en_model_textcat_ngn==0.1.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.5->en_model_textcat_ngn==0.1.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.5->en_model_textcat_ngn==0.1.0) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.5->en_model_textcat_ngn==0.1.0) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.5->en_model_textcat_ngn==0.1.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.5->en_model_textcat_ngn==0.1.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.5->en_model_textcat_ngn==0.1.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.5->en_model_textcat_ngn==0.1.0) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.5->en_model_textcat_ngn==0.1.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.5->en_model_textcat_ngn==0.1.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.5->en_model_textcat_ngn==0.1.0) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.5->en_model_textcat_ngn==0.1.0) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.5->en_model_textcat_ngn==0.1.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.5->en_model_textcat_ngn==0.1.0) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.5->en_model_textcat_ngn==0.1.0) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.5->en_model_textcat_ngn==0.1.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.5->en_model_textcat_ngn==0.1.0) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.5->en_model_textcat_ngn==0.1.0) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.5->en_model_textcat_ngn==0.1.0) (1.26.4)\n",
            "Requirement already satisfied: transformers<4.50.0,>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers<1.4.0,>=1.3.8->en_model_textcat_ngn==0.1.0) (4.48.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers<1.4.0,>=1.3.8->en_model_textcat_ngn==0.1.0) (2.5.1+cu124)\n",
            "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers<1.4.0,>=1.3.8->en_model_textcat_ngn==0.1.0) (0.9.1)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.5->en_model_textcat_ngn==0.1.0) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.5->en_model_textcat_ngn==0.1.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.5->en_model_textcat_ngn==0.1.0) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.5->en_model_textcat_ngn==0.1.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.5->en_model_textcat_ngn==0.1.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.5->en_model_textcat_ngn==0.1.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.5->en_model_textcat_ngn==0.1.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.5->en_model_textcat_ngn==0.1.0) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.5->en_model_textcat_ngn==0.1.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.5->en_model_textcat_ngn==0.1.0) (0.1.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers<1.4.0,>=1.3.8->en_model_textcat_ngn==0.1.0) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers<1.4.0,>=1.3.8->en_model_textcat_ngn==0.1.0) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers<1.4.0,>=1.3.8->en_model_textcat_ngn==0.1.0) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers<1.4.0,>=1.3.8->en_model_textcat_ngn==0.1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers<1.4.0,>=1.3.8->en_model_textcat_ngn==0.1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers<1.4.0,>=1.3.8->en_model_textcat_ngn==0.1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers<1.4.0,>=1.3.8->en_model_textcat_ngn==0.1.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers<1.4.0,>=1.3.8->en_model_textcat_ngn==0.1.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers<1.4.0,>=1.3.8->en_model_textcat_ngn==0.1.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers<1.4.0,>=1.3.8->en_model_textcat_ngn==0.1.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers<1.4.0,>=1.3.8->en_model_textcat_ngn==0.1.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers<1.4.0,>=1.3.8->en_model_textcat_ngn==0.1.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers<1.4.0,>=1.3.8->en_model_textcat_ngn==0.1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers<1.4.0,>=1.3.8->en_model_textcat_ngn==0.1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers<1.4.0,>=1.3.8->en_model_textcat_ngn==0.1.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers<1.4.0,>=1.3.8->en_model_textcat_ngn==0.1.0) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy-transformers<1.4.0,>=1.3.8->en_model_textcat_ngn==0.1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->spacy-transformers<1.4.0,>=1.3.8->en_model_textcat_ngn==0.1.0) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers<1.4.0,>=1.3.8->en_model_textcat_ngn==0.1.0) (0.28.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers<1.4.0,>=1.3.8->en_model_textcat_ngn==0.1.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers<1.4.0,>=1.3.8->en_model_textcat_ngn==0.1.0) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers<1.4.0,>=1.3.8->en_model_textcat_ngn==0.1.0) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy-transformers<1.4.0,>=1.3.8->en_model_textcat_ngn==0.1.0) (0.5.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.5->en_model_textcat_ngn==0.1.0) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.5->en_model_textcat_ngn==0.1.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.5->en_model_textcat_ngn==0.1.0) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.5->en_model_textcat_ngn==0.1.0) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.5->en_model_textcat_ngn==0.1.0) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.5->en_model_textcat_ngn==0.1.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.5->en_model_textcat_ngn==0.1.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.5->en_model_textcat_ngn==0.1.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.5->en_model_textcat_ngn==0.1.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.5->en_model_textcat_ngn==0.1.0) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.5->en_model_textcat_ngn==0.1.0) (0.1.2)\n",
            "Building wheels for collected packages: en_model_textcat_ngn\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_model_textcat_ngn\")\n",
        "\n",
        "#you can use your model then"
      ],
      "metadata": {
        "id": "9b6qapYyEWWt"
      },
      "id": "9b6qapYyEWWt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6b0e4828",
      "metadata": {
        "id": "6b0e4828"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook provided a detailed walkthrough for fine-tuning a SciBERT model to detect automatically generated research abstracts using spaCy. We covered:\n",
        "\n",
        "- Data loading and preparation\n",
        "- Splitting the dataset and converting it to spaCy's binary format\n",
        "- Configuring the pipeline with a Transformer and text classification component\n",
        "- Training, evaluating, and performing inference with the model\n",
        "- Packaging and deploying the trained model\n",
        "\n",
        "Each step has been annotated to ensure transparency and reproducibility of the methodology. Feel free to adapt and expand these notes for further experiments."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lMNWUjUHDsom"
      },
      "id": "lMNWUjUHDsom",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}